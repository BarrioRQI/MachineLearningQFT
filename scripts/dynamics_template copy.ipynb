{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\BCimr\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "#! /usr/bin/python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from scipy.constants import c, hbar, elementary_charge, Boltzmann\n",
    "from sklearn import metrics\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('c:\\\\Users\\\\BCimr\\\\OneDrive - University of Waterloo\\\\ECE\\\\3A\\\\Barrio-RQI\\\\MachineLearningQFT\\\\utils')\n",
    "sys.path.append('c:\\\\Users\\\\BCimr\\\\OneDrive - University of Waterloo\\\\ECE\\\\3A\\\\Barrio-RQI\\\\MachineLearningQFT\\\\scripts\\\\experiment_params')\n",
    "import utilities as u\n",
    "import dynamics as d\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import detector_position_regression_params_v2 as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LPYD = p.LPYD[1:,:]                            # Remove first row \n",
    "nz = np.nonzero(LPYD[:,2].astype(float))[0]  # Find rows with non-zero probability\n",
    "LPYD = LPYD[nz,:]                            # Isolate Rows with non-zero probability\n",
    "\n",
    "DupYLabels = LPYD[:,1]          # Extract the y labels from LPYD (with duplicates)\n",
    "ylabels = list(DupYLabels)                    # Remove the duplicates\n",
    "#for i in DupYLabels: \n",
    "#    if i not in ylabels: \n",
    "#        ylabels.append(i)\n",
    "\n",
    "plist = LPYD[:,2].astype(float) # Extract probabilities from LYPD\n",
    "ptot  = sum(plist)              # Compute their sum\n",
    "plist = plist/ptot              # Normalize them\n",
    "cases = len(plist)              # Number of cases being considered\n",
    "\n",
    "ylist  = LPYD[:,3]  # Extract y-values from LPYD (with duplicates)    \n",
    "Blist = LPYD[:,4].astype(int)      # Extract Boudary conditions from LPYD\n",
    "Dlist = LPYD[:,5].astype(int)      # Extract Distances from LPYD\n",
    "TempList = LPYD[:,6].astype(float) # Extract Distances from LPYD\n",
    "SmearList = LPYD[:,7] # Extract Distances from LPYD\n",
    "DimList = LPYD[:,8].astype(int) # Extract Distances from LPYD\n",
    "if LPYD.shape[1] >= 10:\n",
    "    LatLenList = LPYD[:,9].astype(int)\n",
    "else:\n",
    "    LatLenList = [p.latlen]*len(DimList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position_regression_v2_ntom=1e8_T=0p0K_nH1=32_n_epochs=20_l2reg=1e-4p0_lr=1e-3p0\n",
      "           0      1  2      3  4         5    6         7  8\n",
      "0          0      0  1  0.008  1   0.75564  0.0  gaussian  1\n",
      "1          1      1  1  0.009  1  0.917513  0.0  gaussian  1\n",
      "2          2      2  1  0.007  1  0.689842  0.0  gaussian  1\n",
      "3          3      3  1  0.008  1  0.827548  0.0  gaussian  1\n",
      "4          4      4  1  0.003  1  0.276358  0.0  gaussian  1\n",
      "...      ...    ... ..    ... ..       ...  ...       ... ..\n",
      "49995  49995  49995  1  0.005  1  0.472747  0.0  gaussian  1\n",
      "49996  49996  49996  1   0.01  1  0.972484  0.0  gaussian  1\n",
      "49997  49997  49997  1  0.003  1  0.282295  0.0  gaussian  1\n",
      "49998  49998  49998  1   0.01  1  0.991762  0.0  gaussian  1\n",
      "49999  49999  49999  1  0.001  1  0.058144  0.0  gaussian  1\n",
      "\n",
      "[50000 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(p.experiment_name)\n",
    "print(pd.DataFrame(LPYD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0            1              2              3         4          5  \\\n",
      "0  $\\sigma$       $mc^2$     $\\omega_D$        \\lambda  $T_mean$  $\\DeltaT$   \n",
      "1     0.018  100000000.0  10000000000.0  10000000000.0       0.0        0.0   \n",
      "2  5.092958     0.001179       0.117891       0.117891       0.0          0   \n",
      "\n",
      "        6  \n",
      "0  latlen  \n",
      "1     100  \n",
      "2     100  \n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame([[r'$\\sigma$', '$mc^2$', '$\\omega_D$', '\\lambda', '$T_mean$', '$\\DeltaT$', 'latlen'], \n",
    "                    [p.sigma*p.L0, p.mcc*p.E0/hbar, p.wD*p.E0/hbar, p.lam*p.E0/hbar, p.Tmean*p.Temp0, p.TDev*p.Temp0, p.latlen], \n",
    "                    [p.sigma, p.mcc, p.wD, p.lam, p.Tmean, p.TDev, p.latlen]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1e-13 1e-12 1e-11 1e-10 1e-09 1e-08]\n",
      "[0.00848 0.0848 0.848 8.48 84.8 848]\n"
     ]
    }
   ],
   "source": [
    "print(p.plot_times_max*p.T0)\n",
    "print(p.plot_times_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0             1    2       3              4       5             6  \\\n",
      "0  n_samples  PCA var keep  nH1   L2reg  Learning rate  Epochs  Measurements   \n",
      "1          1             1   32  0.0001          0.001      20            32   \n",
      "\n",
      "             7  \n",
      "0        N_tom  \n",
      "1  100000000.0  \n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame([['n_samples', 'PCA var keep', 'nH1', 'L2reg', 'Learning rate', 'Epochs', 'Measurements', 'N_tom'], \n",
    "                    [p.n_samples, p.PCA_var_keep, p.nH1, p.L2reg, p.learning_rate, p.n_epochs, p.measurements_per_window, p.n_tom]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getcwd() + \"\\\\Data\\\\\" + p.experiment_name\n",
    "try:\n",
    "    os.mkdir(data_dir)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_with_l2_reg(model, lambda_value, regression):\n",
    "    def loss(y_true, y_pred):\n",
    "        eps = 10**(-10) # to prevent the logs from diverging\n",
    "        if regression:\n",
    "            cross_entropy = tf.losses.mean_squared_error(y_true,y_pred)\n",
    "        else:\n",
    "            cross_entropy = tf.reduce_mean(tf.reduce_sum( -y_true * tf.log(y_pred+eps), reduction_indices=[1]))\n",
    "        l2_loss = tf.add_n([tf.nn.l2_loss(w) for w in model.trainable_weights])\n",
    "        return cross_entropy + lambda_value*l2_loss\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x, ylabels, \n",
    "    LO=1, Regression=False, reg_error='mae',\n",
    "    nH1=0, learning_rate=1e-3, \n",
    "    L2reg=1e-2, f_train=0.75, \n",
    "    f_valid=0.2, n_epochs=10, \n",
    "    minibatch_size=256, verbose=True):\n",
    "    ### DEFINE THE NETWORK ARCHITECTURE ###\n",
    "    nI = x.shape[1]-LO\n",
    "    nO = 1 if Regression else len(ylabels)\n",
    "\n",
    "    model_list = [\n",
    "        tf.keras.layers.Flatten(\n",
    "            input_shape=(nI,)\n",
    "            ), \n",
    "        tf.keras.layers.Dense(\n",
    "            nO, \n",
    "            activation=tf.keras.layers.ReLU() if Regression else tf.keras.layers.Softmax(), \n",
    "            kernel_initializer='random_normal',\n",
    "            bias_initializer='zeros'\n",
    "        )]\n",
    "    if nH1 > 0:\n",
    "        model_list.insert(1, tf.keras.layers.Dense(\n",
    "            nH1, \n",
    "            activation=tf.keras.layers.ReLU(), \n",
    "            kernel_initializer='random_normal',\n",
    "            bias_initializer='zeros'\n",
    "            ))\n",
    "    model = tf.keras.Sequential(model_list)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    loss = custom_loss_with_l2_reg(model, L2reg, Regression)\n",
    "    model.compile(\n",
    "        optimizer=optimizer, \n",
    "        loss=loss, \n",
    "        metrics=[reg_error] if Regression else ['accuracy'])\n",
    "\n",
    "    n_train = int(x.shape[0]*(f_train+f_valid))\n",
    "    n_test = x.shape[0] - n_train\n",
    "    x_train = x[:n_train, :-LO]\n",
    "    x_test = x[n_train:n_train+n_test, :-LO]\n",
    "    y_train = x[:n_train, -LO:]\n",
    "    y_test = x[n_train:n_train+n_test, -LO:]\n",
    "\n",
    "    #prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "    #end_step = np.ceil(n_train / minibatch_size).astype(np.int32) * n_epochs\n",
    "    #pruning_params = {'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "    #        initial_sparsity=0.0,\n",
    "    #        final_sparsity=0.60, \n",
    "    #        begin_step=0, \n",
    "    #        end_step=end_step)}\n",
    "    #model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "    #model_for_pruning.compile(\n",
    "    #    optimizer=optimizer, \n",
    "    #    loss=loss, \n",
    "    #    metrics=['mae'] if Regression else ['accuracy'])\n",
    "    #logdir = tempfile.mkdtemp()\n",
    "    #callbacks = [\n",
    "    #    tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "    #    tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "    #]\n",
    "\n",
    "    print('compiling model')\n",
    "    if not Regression:\n",
    "        y_train_OH = tf.keras.utils.to_categorical(y_train, num_classes=nO)\n",
    "        y_test_OH = tf.keras.utils.to_categorical(y_test, num_classes=nO)\n",
    "\n",
    "    print('training model')\n",
    "    history = model.fit(\n",
    "        x_train, \n",
    "        y_train if Regression else y_train_OH, \n",
    "        epochs=n_epochs, \n",
    "        batch_size=minibatch_size, \n",
    "        verbose=1, \n",
    "        validation_split=f_valid/(f_valid+f_train))\n",
    "        #callbacks=callbacks)\n",
    "\n",
    "    if Regression:\n",
    "        accs = [25, 10, 5, 4, 3, 2, 1]\n",
    "        tolerances = [0.02*i - 0.01 for i in accs]\n",
    "        y_pred = model.predict(x_test)\n",
    "        accuracy = [np.sum(np.abs(y_test - y_pred) < tol)/y_test.shape[0] for tol in tolerances]\n",
    "        if verbose:\n",
    "            print(accuracy)\n",
    "        return accuracy\n",
    "    else:\n",
    "        val_acc = history.history['val_acc'][-1]\n",
    "        y_pred = model.predict(x_test)\n",
    "        confusion_matrix = metrics.confusion_matrix(y_test_OH.argmax(axis=1), y_pred.argmax(axis=1), labels=list(range(len(ylabels))))\n",
    "        if verbose:\n",
    "            print(confusion_matrix)\n",
    "            print(val_acc)\n",
    "        return (val_acc, confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hamiltonian represented by symplectic form F, where F is in basis (q1,..., qN, qD, p1, ..., pN, pD)\n",
    "#if p.precompute_hamiltonians:\n",
    "#    Flist_dynamic = [0]*cases         # List of Hamiltonians for dynamics, for each scenario\n",
    "#    Flist_thermal = [0]*cases         # List of Hamiltonians for thermality\n",
    "#    for k in range(cases):\n",
    "#        Flist_dynamic[k], Flist_thermal[k] = \\\n",
    "#            d.get_symplectic_generator(p.wD,p.mcc,p.lam,LatLenList[k],p.sigma,Blist[k],Dlist[k],dim=DimList[k],smearing=SmearList[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_probe_trajectories(\n",
    "    wD,mcc,lam,sigma,\n",
    "    Dlist,\n",
    "    LatLenList,\n",
    "    Blist,\n",
    "    TempList,\n",
    "    DimList,\n",
    "    SmearList,\n",
    "    TDev,\n",
    "    tmin,\n",
    "    tmax,\n",
    "    t_index,\n",
    "    measurements_per_window,\n",
    "    cases,\n",
    "    n_samples,\n",
    "    Regression, \n",
    "    Gsignal=1):\n",
    "\n",
    "    probe_state_0 = d.InitializeProbeState('Ground')\n",
    "    state_list_0 = [0]*cases                                   # List of probe-environment initial states\n",
    "\n",
    "    for k in range(cases):\n",
    "        projectors = d.getProjectorList(Flist_dynamic[k],measurements_per_window,tmin,tmax)\n",
    "    \n",
    "        phi_state_0 = d.ThermalState(Flist_thermal[k],TempList[k]) # Compute the environment's thermal state\n",
    "\n",
    "        \n",
    "        if Blist[k] == 3:                                         # In the signaling case\n",
    "            phi_state_0[0,0]= Gsignal                              # Squeeze the last oscillator\n",
    "            phi_state_0[0,LatLenList[k]]= 0\n",
    "            phi_state_0[LatLenList[k],0]= 0\n",
    "            phi_state_0[LatLenList[k],LatLenList[k]]= 1/Gsignal\n",
    "        state_0 = d.directsum(phi_state_0,probe_state_0)                 # Compute the initial probe-environment state\n",
    "\n",
    "\n",
    "    prepicked_trajectories = [0]*(cases+1)\n",
    "    prepicked_trajectories[cases] = median_trajectory\n",
    "    for k in range(cases):        \n",
    "        dP = projector_list[k] - median_projector      # Difference from median projector\n",
    "        dS = state_list_0[k] - median_state_0      # Difference from median state\n",
    "        #assert(np.count_nonzero(dS) > 0)\n",
    "        trajectory = np.zeros((d1,d2))\n",
    "        for n in range(d1):             # Compute difference from \"median\" trajectory\n",
    "            for r in range(d2):\n",
    "                    trajectory[n,r] += np.trace(dP[n,r] @ median_state_0).real\n",
    "                    trajectory[n,r] += np.trace(median_projector[n,r] @ dS).real\n",
    "                    trajectory[n,r] += np.trace(dP[n,r] @ dS).real\n",
    "                \n",
    "                    trajectory[n,r] += np.trace(proj @ state   - med_proj @ med_state ).real\n",
    "\n",
    "        #assert(np.count_nonzero(trajectory) > 0)\n",
    "        prepicked_trajectories[k] = np.array(trajectory).flatten().real\n",
    "\n",
    "    ### Picking Random initial states for thermal case ###\n",
    "    if Regression == True: \n",
    "        reglist = np.zeros((cases*n_samples,))\n",
    "    else:\n",
    "        reglist = []\n",
    "\n",
    "    if TDev != 0 and t_index == 1:\n",
    "        d1=state_list_0[0].shape[0]\n",
    "        d2=state_list_0[0].shape[1]\n",
    "        BigDSList = np.zeros((cases*n_samples,d1,d2))\n",
    "        for c in range(cases):\n",
    "            for s in range(n_samples):\n",
    "                Temp = d.RTemp(TempList[c],TDev)\n",
    "                phi_state_0 = d.ThermalState(Flist_thermal[c],Temp)\n",
    "                state_0 = d.directsum(phi_state_0,probe_state_0)\n",
    "                BigDSList[c*n_samples+s] = state_0 - median_state_0\n",
    "                if Regression == True: reglist[c*n_samples+s] = Temp\n",
    "    else:\n",
    "        BigDSList = []\n",
    "\n",
    "    if Regression == True and t_index == 1:\n",
    "        minr = min(reglist)\n",
    "        maxr = max(reglist)\n",
    "        reglist = reglist - minr\n",
    "        reglist = reglist/(maxr-minr)\n",
    "        reglist = 0.5*reglist\n",
    "        reglist = reglist + 0.25\n",
    "\n",
    "    return projector_list, median_projector, state_list_0, median_state_0, prepicked_trajectories, median_trajectory, BigDSList, reglist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/6 from time 0.00085 to time 0.00848\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [35]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m     40\u001b[0m     F_dynamic, F_thermal \u001b[38;5;241m=\u001b[39m d\u001b[38;5;241m.\u001b[39mget_symplectic_generator(\n\u001b[0;32m     41\u001b[0m         p\u001b[38;5;241m.\u001b[39mwD,p\u001b[38;5;241m.\u001b[39mmcc,p\u001b[38;5;241m.\u001b[39mlam,LatLenList[k],p\u001b[38;5;241m.\u001b[39msigma,Blist[k],Dlist[k],dim\u001b[38;5;241m=\u001b[39mDimList[k],smearing\u001b[38;5;241m=\u001b[39mSmearList[k])\n\u001b[1;32m---> 43\u001b[0m     projector_list[k] \u001b[38;5;241m=\u001b[39m \u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetProjectorList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF_dynamic\u001b[49m\u001b[43m,\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeasurements_per_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtmax\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m#phi_state_0 = d.ThermalState(F_thermal,TempList[k]) # Compute the environment's thermal state\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m#if Blist[k] == 3:                                          # In the signaling case\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m#    phi_state_0[LatLenList[k],LatLenList[k]]= 1/p.Gsignal\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m#state_list_0[k] = d.directsum(phi_state_0,probe_state_0)                 # Compute the initial probe-environment state\u001b[39;00m\n\u001b[0;32m     53\u001b[0m median_projector \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmedian(projector_list,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# Compute the median projector\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\BCimr\\OneDrive - University of Waterloo\\ECE\\3A\\Barrio-RQI\\MachineLearningQFT\\utils\\dynamics.py:388\u001b[0m, in \u001b[0;36mgetProjectorList\u001b[1;34m(F, N, tmin, tmax)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m--> 388\u001b[0m         ProjList[n,r] \u001b[38;5;241m=\u001b[39m \u001b[43mUdaggerlist\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mProj0\u001b[49m\u001b[43m[\u001b[49m\u001b[43mr\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m@\u001b[39m Ulist[n]\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ProjList\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PCAdData_list = []\n",
    "val_acc = []\n",
    "confusion =[]\n",
    "\n",
    "_range = []\n",
    "if p.rerun_datapoints:\n",
    "    _range = p.reruns\n",
    "else:\n",
    "    _range = range(len(p.plot_times_min))\n",
    "\n",
    "for t_index in _range:\n",
    "    tmax = p.plot_times_max[t_index]\n",
    "    tmin = p.plot_times_min[t_index]\n",
    "    print('Run', str(t_index+1) + '/' + str(len(p.plot_times_max)), 'from time', str(np.round(tmin,5)), 'to time', str(np.round(tmax,5)))\n",
    "\n",
    "    case_time = p.experiment_name + '_time_'+str(np.round(tmax,5))\n",
    "    time_dir = data_dir + '\\\\' + case_time\n",
    "    try:\n",
    "        os.mkdir(time_dir)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "    expdata_name = case_time + '_exp_and_pca_data'\n",
    "    expdata_dir = time_dir + '\\\\' + expdata_name\n",
    "    expdata_exists = False\n",
    "    pcadata_exists = False\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(expdata_dir)\n",
    "    except:\n",
    "        expdata_exists = os.path.isfile(expdata_dir+'\\\\exp_data_all.csv')\n",
    "        pcadata_exists = os.path.isfile(expdata_dir+'\\\\pca_data_all.csv')\n",
    "\n",
    "    projector_list = [0]*cases\n",
    "    prepicked_trajectories = [0]*(cases+1)\n",
    "    state_list_0 = [0]*cases                                   # List of probe-environment initial states\n",
    "    probe_state_0 = d.InitializeProbeState('Ground')\n",
    "\n",
    "    for k in range(1000):\n",
    "        F_dynamic, F_thermal = d.get_symplectic_generator(\n",
    "            p.wD,p.mcc,p.lam,LatLenList[k],p.sigma,Blist[k],Dlist[k],dim=DimList[k],smearing=SmearList[k])\n",
    "\n",
    "        projector_list[k] = d.getProjectorList(F_dynamic,p.measurements_per_window,tmin,tmax)\n",
    "        #phi_state_0 = d.ThermalState(F_thermal,TempList[k]) # Compute the environment's thermal state\n",
    "\n",
    "        #if Blist[k] == 3:                                          # In the signaling case\n",
    "        #    phi_state_0[0,0]= p.Gsignal                              # Squeeze the last oscillator\n",
    "        #    phi_state_0[0,LatLenList[k]]= 0\n",
    "        #    phi_state_0[LatLenList[k],0]= 0\n",
    "        #    phi_state_0[LatLenList[k],LatLenList[k]]= 1/p.Gsignal\n",
    "        #state_list_0[k] = d.directsum(phi_state_0,probe_state_0)                 # Compute the initial probe-environment state\n",
    "\n",
    "    median_projector = np.median(projector_list,axis=0) # Compute the median projector\n",
    "\n",
    "    print(len(projector_list))\n",
    "    print(projector_list[0].shape)\n",
    "    print(len(projector_list))\n",
    "    print(len(projector_list))\n",
    "\n",
    "\n",
    "    median_state_0 = np.median(np.asarray(state_list_0),axis=0)                   # Compute the median probe-environment stat\n",
    "\n",
    "    print(median_projector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/6 from time 0.00085 to time 0.00848\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m     pcadata_exists \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(expdata_dir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mpca_data_all.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pcadata_exists \u001b[38;5;129;01mor\u001b[39;00m p\u001b[38;5;241m.\u001b[39moverwrite \u001b[38;5;129;01mor\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrerun_datapoints:\n\u001b[0;32m     35\u001b[0m     projector_list, median_projector, \\\n\u001b[0;32m     36\u001b[0m         state_list_0, median_state_0, \\\n\u001b[0;32m     37\u001b[0m             prepicked_trajectories, median_trajectory, \\\n\u001b[0;32m     38\u001b[0m                 BigDSList, reglist \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m---> 39\u001b[0m         \u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_probe_trajectories\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m            \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwD\u001b[49m\u001b[43m,\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmcc\u001b[49m\u001b[43m,\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlam\u001b[49m\u001b[43m,\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m            \u001b[49m\u001b[43mDlist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m            \u001b[49m\u001b[43mLatLenList\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m            \u001b[49m\u001b[43mBlist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m            \u001b[49m\u001b[43mTempList\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m            \u001b[49m\u001b[43mDimList\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m            \u001b[49m\u001b[43mSmearList\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m            \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTDev\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m            \u001b[49m\u001b[43mt_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m            \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeasurements_per_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m            \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m            \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRegression\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     exp_data \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39mgenerate_measurement_data(\n\u001b[0;32m     56\u001b[0m         projector_list, \n\u001b[0;32m     57\u001b[0m         median_projector, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     69\u001b[0m         save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[0;32m     70\u001b[0m         path\u001b[38;5;241m=\u001b[39mexpdata_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     71\u001b[0m     exp_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(exp_data)\n",
      "File \u001b[1;32mc:\\Users\\BCimr\\OneDrive - University of Waterloo\\ECE\\3A\\Barrio-RQI\\MachineLearningQFT\\utils\\utilities.py:33\u001b[0m, in \u001b[0;36mget_probe_trajectories\u001b[1;34m(wD, mcc, lam, sigma, Dlist, LatLenList, Blist, TempList, DimList, SmearList, TDev, tmin, tmax, t_index, measurements_per_window, cases, n_samples, Regression, Gsignal)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cases):\n\u001b[0;32m     30\u001b[0m     F_dynamic, F_thermal \u001b[38;5;241m=\u001b[39m d\u001b[38;5;241m.\u001b[39mget_symplectic_generator(\n\u001b[0;32m     31\u001b[0m         wD,mcc,lam,LatLenList[k],sigma,Blist[k],Dlist[k],dim\u001b[38;5;241m=\u001b[39mDimList[k],smearing\u001b[38;5;241m=\u001b[39mSmearList[k])\n\u001b[1;32m---> 33\u001b[0m     projector_list[k] \u001b[38;5;241m=\u001b[39m \u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetProjectorList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF_dynamic\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmeasurements_per_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtmax\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     phi_state_0 \u001b[38;5;241m=\u001b[39m d\u001b[38;5;241m.\u001b[39mThermalState(F_thermal,TempList[k]) \u001b[38;5;66;03m# Compute the environment's thermal state\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m Blist[k] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:                                          \u001b[38;5;66;03m# In the signaling case\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\BCimr\\OneDrive - University of Waterloo\\ECE\\3A\\Barrio-RQI\\MachineLearningQFT\\utils\\dynamics.py:388\u001b[0m, in \u001b[0;36mgetProjectorList\u001b[1;34m(F, N, tmin, tmax)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m--> 388\u001b[0m         ProjList[n,r] \u001b[38;5;241m=\u001b[39m \u001b[43mUdaggerlist\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mProj0\u001b[49m\u001b[43m[\u001b[49m\u001b[43mr\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m@\u001b[39m Ulist[n]\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ProjList\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PCAdData_list = []\n",
    "val_acc = []\n",
    "confusion =[]\n",
    "\n",
    "_range = []\n",
    "if p.rerun_datapoints:\n",
    "    _range = p.reruns\n",
    "else:\n",
    "    _range = range(len(p.plot_times_min))\n",
    "\n",
    "for t_index in _range:\n",
    "    tmax = p.plot_times_max[t_index]\n",
    "    tmin = p.plot_times_min[t_index]\n",
    "    print('Run', str(t_index+1) + '/' + str(len(p.plot_times_max)), 'from time', str(np.round(tmin,5)), 'to time', str(np.round(tmax,5)))\n",
    "\n",
    "    case_time = p.experiment_name + '_time_'+str(np.round(tmax,5))\n",
    "    time_dir = data_dir + '\\\\' + case_time\n",
    "    try:\n",
    "        os.mkdir(time_dir)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "    expdata_name = case_time + '_exp_and_pca_data'\n",
    "    expdata_dir = time_dir + '\\\\' + expdata_name\n",
    "    expdata_exists = False\n",
    "    pcadata_exists = False\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(expdata_dir)\n",
    "    except:\n",
    "        expdata_exists = os.path.isfile(expdata_dir+'\\\\exp_data_all.csv')\n",
    "        pcadata_exists = os.path.isfile(expdata_dir+'\\\\pca_data_all.csv')\n",
    "\n",
    "    if not pcadata_exists or p.overwrite or p.rerun_datapoints:\n",
    "        projector_list, median_projector, \\\n",
    "            state_list_0, median_state_0, \\\n",
    "                prepicked_trajectories, median_trajectory, \\\n",
    "                    BigDSList, reglist = \\\n",
    "            u.get_probe_trajectories(\n",
    "                p.wD,p.mcc,p.lam,p.sigma,\n",
    "                Dlist,\n",
    "                LatLenList,\n",
    "                Blist,\n",
    "                TempList,\n",
    "                DimList,\n",
    "                SmearList,\n",
    "                p.TDev,\n",
    "                tmin,\n",
    "                tmax,\n",
    "                t_index,\n",
    "                p.measurements_per_window, \n",
    "                cases, \n",
    "                p.n_samples,  \n",
    "                p.Regression)\n",
    "        exp_data = u.generate_measurement_data(\n",
    "            projector_list, \n",
    "            median_projector, \n",
    "            prepicked_trajectories, \n",
    "            median_state_0, \n",
    "            BigDSList,\n",
    "            cases, \n",
    "            p.n_samples, \n",
    "            p.measurements_per_window, \n",
    "            p.n_tom, \n",
    "            p.Tdev,\n",
    "            ylist,\n",
    "            p.Regression, \n",
    "            reglist,\n",
    "            save=False, \n",
    "            path=expdata_dir + '\\\\')\n",
    "        exp_data = np.asarray(exp_data)\n",
    "    #elif not pcadata_exists:\n",
    "    #    print('loading experimental simulation data')\n",
    "    #    exp_data = np.asarray(pd.read_csv(expdata_dir+'\\\\exp_data_all.csv'))\n",
    "\n",
    "    #print('Performing PCA')\n",
    "    #if not pcadata_exists or p.overwrite:\n",
    "        PCAdData = u.run_PCA_on_data(\n",
    "            exp_data, \n",
    "            p.f_train, \n",
    "            p.PCA_var_keep, \n",
    "            cases, \n",
    "            save=True, \n",
    "            path=expdata_dir + '\\\\')\n",
    "    else:   \n",
    "        print('loading PCA data')\n",
    "        PCAdData = np.asarray(pd.read_csv(expdata_dir+'\\\\pca_data_all.csv'))\n",
    "    LO = 1                  # Fraction of variance to be kept after PCA (0 to 1 or 'All') \n",
    "\n",
    "    if p.gather_all_data_before_training:\n",
    "        PCAdData_list.append(PCAdData)\n",
    "    else:\n",
    "        output = train_model(\n",
    "            PCAdData, \n",
    "            ylabels, \n",
    "            LO=LO, \n",
    "            Regression=p.Regression, \n",
    "            nH1=p.nH1, \n",
    "            learning_rate=p.learning_rate, \n",
    "            L2reg=p.L2reg, \n",
    "            f_train=p.f_train, \n",
    "            f_valid=p.f_valid, \n",
    "            n_epochs=p.n_epochs, \n",
    "            minibatch_size=p.minibatch_size, \n",
    "            reg_error='mae')\n",
    "\n",
    "        if p.Regression:\n",
    "            val_acc.append(output)\n",
    "        else:\n",
    "            val_acc.append(output[0])\n",
    "            confusion.append(output[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if p.gather_all_data_before_training:\n",
    "    for t_index in range(len(PCAdData_list)):\n",
    "        tmax = p.plot_times_max[t_index]\n",
    "        tmin = p.plot_times_min[t_index]\n",
    "        print('Run', t_index, 'of', len(p.plot_times_max), 'at time', str(np.round(tmin,5)), 'to time', str(np.round(tmax,5)))\n",
    "\n",
    "        case_time = p.experiment_name + '_time_'+str(np.round(tmax,5))\n",
    "        time_dir = data_dir + '\\\\' + case_time\n",
    "        expdata_name = case_time + '_exp_and_pca_data'\n",
    "        expdata_dir = time_dir + '\\\\' + expdata_name\n",
    "\n",
    "        output = train_model(\n",
    "            PCAdData_list[t_index], \n",
    "            ylabels, \n",
    "            LO=LO, \n",
    "            Regression=p.Regression, \n",
    "            nH1=32, \n",
    "            learning_rate=1e-3, \n",
    "            L2reg=1e-4, \n",
    "            f_train=p.f_train, \n",
    "            f_valid=p.f_valid, \n",
    "            n_epochs=15, \n",
    "            minibatch_size=256, \n",
    "            reg_error='mae')\n",
    "\n",
    "        if p.Regression:\n",
    "            val_acc.append(output)\n",
    "        else:\n",
    "            val_acc.append(output[0])\n",
    "            confusion.append(output[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not p.Regression:\n",
    "    conf = np.zeros((len(confusion[0].flatten()), len(confusion)))\n",
    "    for i, c in enumerate(confusion):\n",
    "        conf[:, i] = confusion[i].flatten()\n",
    "\n",
    "plot_times_max_arr = np.asarray(p.plot_times_max).reshape((-1, 1)).T\n",
    "plot_times_min_arr = np.asarray(p.plot_times_min).reshape((-1, 1)).T\n",
    "if not p.Regression:\n",
    "    val_acc_arr = np.asarray(val_acc).reshape((-1, 1)).T\n",
    "else:\n",
    "    val_acc_arr = np.asarray(val_acc).T\n",
    "\n",
    "\n",
    "if not os.path.isfile(data_dir+'\\\\test_output.csv'):\n",
    "    output = (plot_times_min_arr, plot_times_max_arr, val_acc_arr) if p.Regression else (plot_times_min_arr, plot_times_max_arr, val_acc_arr, conf)\n",
    "    test_result = np.concatenate(output, axis=0)\n",
    "    pd.DataFrame(test_result).to_csv(data_dir+'\\\\test_output.csv')\n",
    "\n",
    "elif p.rerun_datapoints:\n",
    "    previous_output = pd.read_csv(data_dir+'\\\\test_output.csv').to_numpy()[:, 1:]\n",
    "    for i, j in enumerate(p.reruns):\n",
    "        if p.Regression:\n",
    "            previous_output[2:, j] = val_acc[i]\n",
    "            print(previous_output[2:, j])\n",
    "        else:\n",
    "            previous_output[2, j] = val_acc[i]\n",
    "            previous_output[3:, j] = conf[:, i]\n",
    "    pd.DataFrame(previous_output).to_csv(data_dir+'\\\\test_output.csv')\n",
    "\n",
    "elif p.overwrite:\n",
    "    output = (plot_times_min_arr, plot_times_max_arr, val_acc_arr) if p.Regression else (plot_times_min_arr, plot_times_max_arr, val_acc_arr, conf)\n",
    "    test_result = np.concatenate(output, axis=0)\n",
    "    pd.DataFrame(test_result).to_csv(data_dir+'\\\\test_output.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.isfile(data_dir+'\\\\test_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax=plt.subplots()\n",
    "ax.plot(p.plot_times_max*p.a/3e8, val_acc)\n",
    "ax.plot([50*p.a/3e8]*2, [0,1])\n",
    "\n",
    "ax.set_xscale('log')\n",
    "#plt.xlim(0, 1)\n",
    "#plt.ylim(0, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
